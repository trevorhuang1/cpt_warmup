{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "toc: true\n",
    "comments: true\n",
    "layout: post\n",
    "title: 5.3 Computing Bias Lesson\n",
    "description: Computing Bias Lesson\n",
    "type: hacks\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Bias - Hanlun\n",
    "What is it?\n",
    "Computing bias is a bias that exists within algorithms that unfairly discriminate against certain individuals or groups, creating unfair outcomes\n",
    "Computing innovations reflect biases because algorithms and data are influenced by the people who contributed to it, so computing biases can be embedded at every level of development \n",
    "Algorithm bias should be reduced so computer innovations can combat existing user bias\n",
    "Popcorn Hack #1: \n",
    "Which of the following scenarios is an example of computing bias?\n",
    "\n",
    "A) An email filtering system accurately categorizes emails into spam and non-spam based on a diverse set of features, minimizing false positives and false negatives.\n",
    "B) A navigation app provides real-time traffic updates and alternate routes to users, considering various factors such as traffic volume, road closures, and weather conditions.\n",
    "C) An image recognition algorithm identifies objects in photographs with high accuracy, regardless of the gender, ethnicity, or age of the individuals depicted.\n",
    "D) An automated hiring system consistently favors candidates from specific educational institutions and backgrounds, resulting in the exclusion of qualified applicants from diverse backgrounds.\n",
    "\n",
    "\n",
    "## Types of Computing Bias - Trevor\n",
    "Computing bias can be either intentional or unintentional. (ex. Player Bases of casual vs sweaty games, people who use Facebook vs Instagram vs Tiktok, people who use WeChat vs Kakao Talk)\n",
    "One example of this is if an algorithm was developed to hire for a job that was mostly occupied by male employees, that algorithm might favor male employees\n",
    "Another example is the “racist” HP facial recognition feature which couldn’t follow the face of someone with darker skin. This was likely due to the lack of diversity in the training data therefore bias in the data.\n",
    "\n",
    "## Explicit and Implicit Data - Lakshanya\n",
    "Explicit data is information you give an app or site\n",
    "Implicit data is the information it collects based on your activity\n",
    "For example, with Netflix, the explicit data it collects includes your rating of movies (thumbs up, thumbs down) and your personal information. The implicit data it collects includes when you watch, what you binge, and the frequently selected genres. This data collection is also known as user bias. More bias comes in when Netflix shows its exclusives before any of the regular items to gain subscriptions.\n",
    "\n",
    "## Mitigation Strategies: - Matthew\n",
    "Pre-processing: write algorithms to adjust data sets in order to remove bias before using it as an input\n",
    "In-processing: modifying or manipulating your algorithm to make it less prone to bias\n",
    "Post-processing: This type of strategy is comprised of 3 parts, input correction, classifier correction, output correction\n",
    "input correction: very similar to pre-processing strategies, but instead of applying the modifications in the data before the training process, the modifications are applied to the testing data once the model has been trained\n",
    "Classifier correction: adjusts the algorithm to remove discrimination\n",
    "Output correction: directly modifies the output of any algorithm in order to remove discrimination\n",
    "\n",
    "## Hacks:\n",
    "Problem: Biased Predictive Policing Algorithm: A city implements a predictive policing algorithm to allocate law enforcement resources more efficiently. \n",
    "However, concerns arise as community members and activists notice that the algorithm appears to disproportionately target certain neighborhoods, leading to over-policing and potential violations of civil rights.\n",
    "Provide a solution to how this situation can be resolved, and how the computing bias can be removed. Explain which method of mitigation you will use and how it works. \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
